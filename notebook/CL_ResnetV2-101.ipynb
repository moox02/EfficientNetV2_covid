{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1199638-1fa2-421f-b58a-bf8b4d88fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import albumentations as A\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import scikitplot as skplt\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd640d-c568-45cb-9b53-f30e3f50d78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "def seed_all(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f9000-c6a9-4f80-8f02-ba46cadfcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_NAME = \"CL_ResnetV2-101\"\n",
    "SEED = 124 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "MODEL_NAME = \"resnet_v2_101\" \n",
    "IMAGE_SIZE = model_image_size_map.get(MODEL_NAME, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "CLASS_NAMES = ['normal', 'pneumonia', 'COVID-19']\n",
    "\n",
    "model_handle = model_handle_map.get(MODEL_NAME)\n",
    "print(f\"Selected model : {MODEL_NAME} : {model_handle}\")\n",
    "print(f\"Input size     : {IMAGE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fd31d-d35e-4c33-aefc-a138c27e485e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the augmentation policies\n",
    "transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(p=0.5, limit=15),\n",
    "    A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.1, 0.1), brightness_by_max=True),\n",
    "    A.RandomResizedCrop(p=0.5, height=IMAGE_SIZE, width=IMAGE_SIZE, scale=(0.9, 1.1), ratio=(0.05, 1.1), interpolation=0),\n",
    "])\n",
    "\n",
    "# Apply augmentation policies.\n",
    "def aug_fn(image):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"] \n",
    "    return aug_img\n",
    "\n",
    "# Augmentation policies\n",
    "def apply_augmentation(image, label):\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    aug_img.set_shape((IMAGE_SIZE, IMAGE_SIZE, 3))    \n",
    "    return aug_img, label\n",
    "\n",
    "# Preprocess image\n",
    "def preprocess_data(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = image/255.0\n",
    "    image = tf.squeeze(image, 0) \n",
    "    label = tf.squeeze(label, 0) \n",
    "    return image, label\n",
    "\n",
    "# View image from dataset\n",
    "def view_image(ds, col=8, row=2, size=(25,7)):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n",
    "    for images, labels in ds.take(1):\n",
    "        for i in range(col*row):\n",
    "            img_numpy = images[i].numpy()\n",
    "            ax = plt.subplot(row, col, i + 1)\n",
    "            shape = str(images[i].numpy().shape)\n",
    "            plt.imshow(img_numpy)\n",
    "            plt.title(CLASS_NAMES[np.argmax(labels[i].numpy())])\n",
    "            plt.axis(\"off\") \n",
    "    plt.tight_layout\n",
    "    return None\n",
    "\n",
    "# Plot training history\n",
    "def training_history(history):\n",
    "    accuracy = history['accuracy']\n",
    "    val_accuracy = history['val_accuracy']\n",
    "\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(history['loss']))\n",
    "\n",
    "    plt.figure(figsize=(32, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# Parse test images\n",
    "def decode_test(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE], antialias=True)/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c0a1-7b81-4dcb-aa4d-e6aa82fd031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(SEED)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../dataset/clahe/train/',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = CLASS_NAMES,\n",
    "    batch_size = 1,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    interpolation = 'bilinear'\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../dataset/clahe/valid/',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = CLASS_NAMES,\n",
    "    batch_size = 1,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    interpolation = 'bilinear'\n",
    ")\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = (\n",
    "    valid_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793f31d-0b3f-4094-ae7f-111355f67ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer = hub.KerasLayer(model_handle, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), trainable=True)\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917dbe4-5cc7-4e52-a892-f0335cfa8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_scheduler = tfa.optimizers.CyclicalLearningRate( \n",
    "    initial_learning_rate=3e-7,  maximal_learning_rate=7e-3,\n",
    "    step_size=3*(20994//BATCH_SIZE),  \n",
    "    scale_fn=lambda x: 1 / (2.0 ** (x - 1)), \n",
    "    scale_mode='cycle'\n",
    ")\n",
    "    \n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='loss', verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=clr_scheduler) , \n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "    metrics=METRICS\n",
    ")\n",
    "\n",
    "start_training = time.time()\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks=[earlyStopping],\n",
    "    validation_data=valid_ds,\n",
    ")\n",
    "\n",
    "end_training = time.time()\n",
    "print(f\"Time taken to train model : {end_training-start_training} sec\")\n",
    "\n",
    "training_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193ecba-c21d-4f24-822f-52dbc3769004",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_df = pd.read_csv('../dataset/test.csv')\n",
    "tests_df['path'] = '../dataset/clahe/test/'+ tests_df.label + '/' + tests_df.filename\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(tests_df.path) \n",
    "test_ds = test_ds.map(decode_test,num_parallel_calls=AUTOTUNE).batch(len(tests_df))\n",
    "\n",
    "test_index = np.argmax(tests_df[CLASS_NAMES].values, axis=1)\n",
    "test_label = tests_df.label.values\n",
    "\n",
    "start_predict = time.time()\n",
    "\n",
    "test_pred = model.predict(test_ds)\n",
    "pred_index = np.argmax(test_pred, axis=1)\n",
    "pred_label = np.array(CLASS_NAMES)[pred_index]\n",
    "\n",
    "end_predict = time.time()\n",
    "print(f\"Inference time : {end_predict-start_predict} sec\")\n",
    "\n",
    "print(classification_report(test_index, pred_index, target_names=CLASS_NAMES, zero_division=0, digits=4))\n",
    "print('f1_score        :', f1_score(test_index, pred_index, average='micro'))\n",
    "print('accuracy_score  :', accuracy_score(test_index, pred_index))\n",
    "\n",
    "cm = skplt.metrics.plot_confusion_matrix(test_label, pred_label, figsize=(8, 8), normalize=False)\n",
    "roc = skplt.metrics.plot_roc(test_index, test_pred, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69131b-c397-473b-9ca0-a65ef5ffc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_df.to_csv(\"../history/\"+TRAINING_NAME+\".csv\", index=False)\n",
    "model.save_weights(\"../model_weight/\"+TRAINING_NAME+\"_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cba85-13cd-44de-87ac-a4f738603e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
