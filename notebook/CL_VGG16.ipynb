{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1199638-1fa2-421f-b58a-bf8b4d88fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import albumentations as A\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import scikitplot as skplt\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd640d-c568-45cb-9b53-f30e3f50d78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_all(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f9000-c6a9-4f80-8f02-ba46cadfcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_NAME = \"CL_VGG16\"\n",
    "SEED = 124 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "CLASS_NAMES = ['normal', 'pneumonia', 'COVID-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fd31d-d35e-4c33-aefc-a138c27e485e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the augmentation policies\n",
    "transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(p=0.5, limit=15),\n",
    "    A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.1, 0.1), brightness_by_max=True),\n",
    "    A.RandomResizedCrop(p=0.5, height=IMAGE_SIZE, width=IMAGE_SIZE, scale=(0.9, 1.1), ratio=(0.05, 1.1), interpolation=0),\n",
    "])\n",
    "\n",
    "# Apply augmentation policies.\n",
    "def aug_fn(image):\n",
    "    data = {\"image\":image}\n",
    "    aug_data = transforms(**data)\n",
    "    aug_img = aug_data[\"image\"] \n",
    "    return aug_img\n",
    "\n",
    "# Augmentation policies\n",
    "def apply_augmentation(image, label):\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    aug_img.set_shape((IMAGE_SIZE, IMAGE_SIZE, 3))    \n",
    "    return aug_img, label\n",
    "\n",
    "# Preprocess image\n",
    "def preprocess_data(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = image/255.0\n",
    "    image = tf.squeeze(image, 0) \n",
    "    label = tf.squeeze(label, 0) \n",
    "    return image, label\n",
    "\n",
    "# View image from dataset\n",
    "def view_image(ds, col=8, row=2, size=(25,7)):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n",
    "    for images, labels in ds.take(1):\n",
    "        for i in range(col*row):\n",
    "            img_numpy = images[i].numpy()\n",
    "            ax = plt.subplot(row, col, i + 1)\n",
    "            shape = str(images[i].numpy().shape)\n",
    "            plt.imshow(img_numpy)\n",
    "            plt.title(CLASS_NAMES[np.argmax(labels[i].numpy())])\n",
    "            plt.axis(\"off\") \n",
    "    plt.tight_layout\n",
    "    return None\n",
    "\n",
    "# Plot training history\n",
    "def training_history(history):\n",
    "    accuracy = history['accuracy']\n",
    "    val_accuracy = history['val_accuracy']\n",
    "\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(history['loss']))\n",
    "\n",
    "    plt.figure(figsize=(32, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# Parse test images\n",
    "def decode_test(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE], antialias=True)/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c0a1-7b81-4dcb-aa4d-e6aa82fd031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(SEED)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../dataset/clahe/train/',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = CLASS_NAMES,\n",
    "    batch_size = 1,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    interpolation = 'bilinear'\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../dataset/clahe/valid/',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = CLASS_NAMES,\n",
    "    batch_size = 1,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle = True,\n",
    "    seed = SEED,\n",
    "    interpolation = 'bilinear'\n",
    ")\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = (\n",
    "    valid_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024c38d-d556-4c10-845e-65064816d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a dropout layer\n",
    "x = Dropout(0.2)(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917dbe4-5cc7-4e52-a892-f0335cfa8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_scheduler = tfa.optimizers.CyclicalLearningRate( \n",
    "    initial_learning_rate=3e-7,  maximal_learning_rate=7e-3,\n",
    "    step_size=3*(20994//BATCH_SIZE),  \n",
    "    scale_fn=lambda x: 1 / (2.0 ** (x - 1)), \n",
    "    scale_mode='cycle'\n",
    ")\n",
    "    \n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='loss', verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=clr_scheduler) , \n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), \n",
    "    metrics=METRICS\n",
    ")\n",
    "\n",
    "start_training = time.time()\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks=[earlyStopping],\n",
    "    validation_data=valid_ds,\n",
    ")\n",
    "\n",
    "end_training = time.time()\n",
    "print(f\"Time taken to train model : {end_training-start_training} sec\")\n",
    "\n",
    "training_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193ecba-c21d-4f24-822f-52dbc3769004",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_df = pd.read_csv('../dataset/test.csv')\n",
    "tests_df['path'] = '../dataset/clahe/test/'+ tests_df.label + '/' + tests_df.filename\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(tests_df.path) \n",
    "test_ds = test_ds.map(decode_test,num_parallel_calls=AUTOTUNE).batch(len(tests_df))\n",
    "\n",
    "test_index = np.argmax(tests_df[CLASS_NAMES].values, axis=1)\n",
    "test_label = tests_df.label.values\n",
    "\n",
    "start_predict = time.time()\n",
    "\n",
    "test_pred = model.predict(test_ds)\n",
    "pred_index = np.argmax(test_pred, axis=1)\n",
    "pred_label = np.array(CLASS_NAMES)[pred_index]\n",
    "\n",
    "end_predict = time.time()\n",
    "print(f\"Inference time : {end_predict-start_predict} sec\")\n",
    "\n",
    "print(classification_report(test_index, pred_index, target_names=CLASS_NAMES, zero_division=0, digits=4))\n",
    "print('f1_score        :', f1_score(test_index, pred_index, average='micro'))\n",
    "print('accuracy_score  :', accuracy_score(test_index, pred_index))\n",
    "\n",
    "cm = skplt.metrics.plot_confusion_matrix(test_label, pred_label, figsize=(8, 8), normalize=False)\n",
    "roc = skplt.metrics.plot_roc(test_index, test_pred, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69131b-c397-473b-9ca0-a65ef5ffc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_df.to_csv(\"../history/\"+TRAINING_NAME+\".csv\", index=False)\n",
    "model.save_weights(\"../model_weight/\"+TRAINING_NAME+\"_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cba85-13cd-44de-87ac-a4f738603e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
